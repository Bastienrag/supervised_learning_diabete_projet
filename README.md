# Predicting Hospital Readmission Risk for Diabetic Patients  
**Contributor : Bastien Ragueneau**

---

## ðŸ“Œ 1. Business Challenge

Hospital readmissions within 30 days are costly, harmful for patients, and a key indicator of hospital performance.  
Diabetic patients are particularly vulnerable due to complex comorbidities and chronic complications.

**Objective:**  
Build a machine learning model to **predict whether a diabetic patient will be readmitted within 30 days** after discharge.

**Why it matters:**  
- identify high-risk patients early,  
- allow targeted follow-up interventions,  
- reduce avoidable hospital costs,  
- improve patient care and outcomes.

This repository provides a complete, reproducible pipeline for this predictive task.

---

## ðŸ“¦ 2. Dataset Description

Dataset used: **Diabetes 130-US hospitals**  
Source: UCI Machine Learning Repository  

**Initial characteristics:**
- >100,000 hospital encounters  
- >50 features (demographics, ICD-9 diagnoses, medications, hospital stay metrics, A1Câ€¦)  
- Original target variable: `readmitted` (`NO`, `<30`, `>30`)

The raw dataset contains many inconsistencies:
- missing values encoded as `"?"`
- high-cardinality diagnosis codes (ICD-9)
- erratic medication columns
- several useless administrative variables
- mixed data types

A substantial cleaning and transformation effort was required.

---

## ðŸ”§ 3. Data Cleaning & Feature Engineering

### âœ” Cleaning
- Removal of columns with excessive missingness:  
  `weight`, `payer_code`, `medical_specialty`
- Removal of rows where `race = "?"` (~2% of data)
- Replacement or removal of all `"?"` values
- Removal of medication columns with quasi-constant distribution
- Conversion of all fields into numeric types
- Removal of high-cardinality, low-value columns

### âœ” Feature Engineering
- **ICD-9 regrouping** for `diag_1`, `diag_2`, `diag_3`  
  â†’ mapped to meaningful medical categories  
  (e.g., Circulatory System, Diabetes, Infectious Diseasesâ€¦)
- `diag_1`: One-Hot Encoding  
- `diag_2`, `diag_3`: binary comorbidity indicators  
- Encoding of medication variables (insulin, metformin, etc.)
- Transformation of A1C results into ordinal feature (`A1Cresult_cat`)
- Cleaning & encoding of `gender` and `race`
- Creation of binary target:
    readmitted_flag = 1 if readmitted == "<30"
    readmitted_flag = 0 otherwise

### âœ” Final dataset characteristics
- **269,346 rows**
- **â‰ˆ 60 fully numeric features**
- **0 missing values**
- ML-ready format

---

## âš™ï¸ 4. Reproducibility Instructions

### ðŸ Python Version  
Use **Python 3.9+**

---

### ðŸ“‚ 4.1 Dataset placement
Place the cleaned dataset in:
project/
â”‚â”€â”€ data/
â””â”€â”€ diabetes_clean.csv

---

### ðŸ“¦ 4.2 Install dependencies
pip install -r requirements.txt

---

### ðŸš€ 4.3 Run the entire ML pipeline
python main.py

This script:
- loads the dataset  
- preprocesses features  
- trains the optimized Random Forest  
- evaluates performance  
- saves the final model to `models/random_forest_best.joblib`

---

## ðŸ§ª 5. Baseline Model

### Baseline: **Logistic Regression**

**Features:**  
All cleaned numerical features.

**Preprocessing:**  
- StandardScaler  
- No dimensionality reduction  
- No feature selection  

**Baseline Metrics (test set):**
- ROC-AUC: ~0.78  
- Accuracy: ~0.73  

This baseline serves as the reference for performance improvements.

---

## ðŸ”¬ 6. Experiment Tracking

A structured, iterative improvement process was followed:

| Iteration | Modification | Model | ROC-AUC | Notes |
|----------|--------------|--------|----------|-------|
| 1 | Baseline logistic regression | LogReg | ~0.78 | Baseline reference |
| 2 | Added Decision Tree | DecisionTree | ~0.83 | Captures nonlinearity |
| 3 | Random Forest (default params) | RF | ~0.94 | Large improvement |
| 4 | Full feature engineering | RF | ~0.97 | Strong leap from clean data |
| 5 | Hyperparameter tuning (GridSearchCV) | **RF (optimized)** | **0.9903** | Best model |
| 6 | XGBoost test | XGB | ~0.985 | Very strong, but below RF |

### ðŸŽ¯ Best model  
**Optimized Random Forest**  
Best hyperparameters (found via GridSearchCV):
n_estimators = 300
max_depth = None
min_samples_split = 2
min_samples_leaf = 1
class_weight = "balanced"

---

## ðŸ† 7. Final Model Performance (Test Set)

| Metric | Score |
|--------|--------|
| Accuracy | 0.996 |
| Precision | 0.999 |
| Recall | 0.963 |
| F1-score | 0.981 |
| ROC-AUC | 0.998 |

### Why performance is high (and realistic)
- Very strong signal in ICD-9 codes and hospitalization history  
- Excellent feature engineering  
- Dataset size (269k rows) favors ensemble trees  
- Cross-validated hyperparameter tuning  
- Fully numerical, clean, noise-free feature matrix  

These scores align with best Kaggle solutions on this dataset.

---

## ðŸ“ 8. Project Structure

project/
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ eda.ipynb
â”‚â”€â”€ diabetes_modeling.ipynb
â”‚â”€â”€ main.py
â”‚â”€â”€ models/
â”‚ â””â”€â”€ random_forest_best.joblib
â”‚â”€â”€ data/
â”‚ â””â”€â”€ diabetes_clean.csv
â”‚ â””â”€â”€ diabetic_data.csv
â”‚ â””â”€â”€ IDS_mapping.csv

---

## ðŸ“¬ Contact  
**Bastien Ragueneau**  
Albert School â€“ Business & Data (2025)